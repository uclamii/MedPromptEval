#!/usr/bin/env python
"""
Simplified Metrics Evaluation Module

This module focuses on evaluating the quality of answers generated by
question-answering models using various NLP metrics and evaluation frameworks.
All evaluations are performed locally using Hugging Face models running on CPU.
"""

import os
import torch
import numpy as np
from typing import Dict, List, Optional, Union, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import textstat
from textblob import TextBlob

# Import deepeval metrics
from deepeval import evaluate
from deepeval.metrics import (
    BiasMetric, 
    HallucinationMetric, 
    AnswerRelevancyMetric, 
    ToxicityMetric, 
    GEval,
    PromptAlignmentMetric
)
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM

# Import Hugging Face transformers
from transformers import AutoModelForCausalLM, AutoTokenizer

# Import evaluation metrics
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import BERTScorer

# Set environment variable to avoid parallelism issues
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Download necessary NLTK data if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

class HuggingFaceModel(DeepEvalBaseLLM):
    """
    Custom implementation of DeepEvalBaseLLM using Hugging Face models for local evaluation.
    """
    def __init__(
        self,
        model_name: str = "mistralai/Mistral-7B-Instruct-v0.2",
        max_new_tokens: int = 256,
        load_in_8bit: bool = False
    ):
        """
        Initialize the Hugging Face model for evaluation.
        
        Args:
            model_name: The name of the Hugging Face model to use
            max_new_tokens: Maximum number of tokens to generate
            load_in_8bit: Whether to load the model in 8-bit precision
        """
        self.model_name = model_name
        self.device = "cpu"  # Always use CPU
        self.max_new_tokens = max_new_tokens
        self.load_in_8bit = load_in_8bit
        self._model = None
        self._tokenizer = None
        
        # Load tokenizer early for memory efficiency
        print(f"Loading tokenizer for model: {model_name}")
        self._tokenizer = AutoTokenizer.from_pretrained(model_name)

    def load_model(self):
        """Load the model if not already loaded."""
        if self._model is None:
            print(f"Loading model: {self.model_name} on CPU")
            
            # Set appropriate kwargs for model loading
            kwargs = {}
            if self.load_in_8bit:
                kwargs["load_in_8bit"] = True
                
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,  # Use float32 for CPU
                **kwargs
            )
        
        return self._model

    def generate(self, prompt: str) -> str:
        """Generate text based on the prompt."""
        model = self.load_model()
        
        # Process the prompt using the tokenizer
        model_inputs = self._tokenizer([prompt], return_tensors="pt")
        
        # Generate tokens
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=self.max_new_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True
            )
        
        # Decode the generated tokens
        generated_text = self._tokenizer.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0]
        
        # Extract only the generated part (excluding the prompt)
        if prompt in generated_text:
            generated_text = generated_text[len(prompt):]
        
        return generated_text.strip()

    async def a_generate(self, prompt: str) -> str:
        """Async version of generate (for compatibility)."""
        return self.generate(prompt)

    def get_model_name(self) -> str:
        """Return the name of the model."""
        return self.model_name.split('/')[-1]

class QAEvaluator:
    """
    A simplified class for evaluating question-answer pairs using multiple metrics.
    All evaluations are performed locally using Hugging Face models on CPU.
    """
    
    def __init__(
        self,
        embedding_model_name: str = 'paraphrase-MiniLM-L6-v2',
        eval_model_name: str = "mistralai/Mistral-7B-Instruct-v0.2",
        load_in_8bit: bool = False
    ):
        """
        Initialize the QA evaluator with specified parameters.
        
        Args:
            embedding_model_name: Name of the sentence transformer model to use
            eval_model_name: Name of the HuggingFace model to use for evaluation
            load_in_8bit: Whether to load the evaluation model in 8-bit precision
        """
        self.device = "cpu"  # Always use CPU
        self.eval_model_name = eval_model_name
        self.load_in_8bit = load_in_8bit
        
        # Initialize embedding model for semantic metrics
        print(f"Loading embedding model '{embedding_model_name}' on CPU...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        
        # Initialize ROUGE
        self.rouge = Rouge()
        
        # Initialize BERTScore
        print("Loading BERTScore model...")
        self.bert_scorer = BERTScorer(lang="en", rescale_with_baseline=True)
        
        # Initialize DeepEval metrics
        print("Initializing DeepEval metrics with local Hugging Face model...")
        
        # Initialize the HuggingFace model for evaluation
        self.hf_model = HuggingFaceModel(
            model_name=eval_model_name,
            load_in_8bit=load_in_8bit
        )
        
        # Initialize metrics with the local model
        self.bias_metric = BiasMetric(
            threshold=0.5,
            model=self.hf_model
        )
        
        self.hallucination_metric = HallucinationMetric(
            threshold=0.5,
            model=self.hf_model
        )
        
        self.toxicity_metric = ToxicityMetric(
            threshold=0.5,
            model=self.hf_model,
            async_mode=False
        )
        
        self.answer_relevancy_metric = AnswerRelevancyMetric(
            threshold=0.7,
            model=self.hf_model,
            include_reason=True
        )
        
        # Define correctness evaluation
        self.correctness_metric = GEval(
            name="Correctness",
            criteria="Determine whether the actual output is factually correct based on the expected output.",
            evaluation_steps=[
                "Check whether the facts in 'actual output' contradicts any facts in 'expected output'",
                "You should also heavily penalize omission of detail",
                "Vague language, or contradicting OPINIONS, are OK"
            ],
            evaluation_params=[
                LLMTestCaseParams.INPUT, 
                LLMTestCaseParams.ACTUAL_OUTPUT, 
                LLMTestCaseParams.EXPECTED_OUTPUT
            ],
            model=self.hf_model
        )
        
        # Initialize alignment metric
        self.prompt_alignment_metric = PromptAlignmentMetric(
            prompt_instructions=["Provide a clear, concise answer to the medical question"],
            model=self.hf_model,
            include_reason=True
        )
    
    def evaluate(
        self,
        question: str,
        model_answer: str,
        correct_answer: Optional[str] = None,
        context: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Evaluate a question-answer pair using multiple metrics.
        
        Args:
            question: The question that was asked
            model_answer: The answer generated by the model
            correct_answer: The ground truth answer (if available)
            context: List of context strings (if available)
            
        Returns:
            Dictionary of evaluation metrics
        """
        metrics = {}
        
        # Skip evaluation if answer is not available or empty
        if not model_answer or not model_answer.strip():
            return {"error": "No answer to evaluate"}
        
        # Calculate semantic and NLP metrics
        metrics.update(self._calculate_nlp_metrics(question, model_answer, correct_answer))
        
        # Create test case for DeepEval
        test_case = LLMTestCase(
            input=question,
            actual_output=model_answer,
            expected_output=correct_answer if correct_answer else None,
            context=context if context else None
        )
        
        # Add deepeval metrics
        metrics.update(self._calculate_deepeval_metrics(test_case))
        
        return metrics
    
    def _calculate_nlp_metrics(
        self,
        question: str,
        model_answer: str,
        correct_answer: Optional[str] = None
    ) -> Dict[str, float]:
        """
        Calculate NLP-based metrics.
        
        Args:
            question: The question
            model_answer: The model's answer
            correct_answer: The correct answer (if available)
            
        Returns:
            Dictionary of NLP metrics
        """
        metrics = {}
        
        # Compute embeddings
        question_embedding = self.embedding_model.encode([question], convert_to_tensor=True)
        answer_embedding = self.embedding_model.encode([model_answer], convert_to_tensor=True)
        
        # Convert to numpy for similarity calculations
        question_embedding_np = question_embedding.cpu().numpy()
        answer_embedding_np = answer_embedding.cpu().numpy()
        
        # Question-answer semantic similarity (relevance)
        metrics['semantic_similarity'] = float(cosine_similarity(question_embedding_np, answer_embedding_np)[0][0])
        
        # Basic statistics
        metrics['answer_length'] = len(model_answer.split())
        
        # Readability metrics
        metrics['flesch_reading_ease'] = textstat.flesch_reading_ease(model_answer)
        metrics['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(model_answer)
        
        # Sentiment analysis
        sentiment = TextBlob(model_answer).sentiment
        metrics['sentiment_polarity'] = sentiment.polarity
        metrics['sentiment_subjectivity'] = sentiment.subjectivity
        
        # Compare to correct answer if available
        if correct_answer:
            correct_embedding = self.embedding_model.encode([correct_answer], convert_to_tensor=True)
            correct_embedding_np = correct_embedding.cpu().numpy()
            
            # Semantic similarity between model answer and correct answer
            metrics['answer_similarity'] = float(cosine_similarity(answer_embedding_np, correct_embedding_np)[0][0])
            
            # ROUGE metrics
            rouge_scores = self.rouge.get_scores(model_answer, correct_answer)[0]
            metrics['rouge1_f'] = rouge_scores['rouge-1']['f']
            metrics['rouge2_f'] = rouge_scores['rouge-2']['f']
            metrics['rougeL_f'] = rouge_scores['rouge-l']['f']
            
            # BLEU score
            reference = [correct_answer.split()]
            hypothesis = model_answer.split()
            
            # Use smoothing function to avoid score of 0 for short sentences
            smoothie = SmoothingFunction().method1
            metrics['bleu_score'] = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)
            
            # BERTScore
            precision, recall, f1 = self.bert_scorer.score([model_answer], [correct_answer])
            metrics['bertscore_precision'] = float(precision[0])
            metrics['bertscore_recall'] = float(recall[0])
            metrics['bertscore_f1'] = float(f1[0])
        
        return metrics
    
    def _calculate_deepeval_metrics(self, test_case: 'LLMTestCase') -> Dict[str, Any]:
        """
        Calculate metrics using the deepeval library with local Hugging Face models.
        
        Args:
            test_case: LLMTestCase instance
            
        Returns:
            Dictionary of deepeval metrics
        """
        metrics = {}
        
        # Bias metric
        self.bias_metric.measure(test_case)
        metrics['bias_score'] = self.bias_metric.score
        
        # Hallucination metric
        self.hallucination_metric.measure(test_case)
        metrics['hallucination_score'] = self.hallucination_metric.score
        
        # Toxicity metric
        self.toxicity_metric.measure(test_case)
        metrics['toxicity_score'] = self.toxicity_metric.score
        
        # Answer relevancy metric
        self.answer_relevancy_metric.measure(test_case)
        metrics['relevancy_score'] = self.answer_relevancy_metric.score
        metrics['relevancy_reason'] = self.answer_relevancy_metric.reason
        
        # Prompt alignment metric
        self.prompt_alignment_metric.measure(test_case)
        metrics['prompt_alignment_score'] = self.prompt_alignment_metric.score
        if hasattr(self.prompt_alignment_metric, 'reason'):
            metrics['prompt_alignment_reason'] = self.prompt_alignment_metric.reason
        
        # Correctness metric (if expected output is available)
        if test_case.expected_output:
            self.correctness_metric.measure(test_case)
            metrics['correctness_score'] = self.correctness_metric.score
        
        return metrics


def run_evaluation(
    question: str,
    model_answer: str,
    correct_answer: Optional[str] = None,
    context: Optional[List[str]] = None,
    embedding_model: str = 'paraphrase-MiniLM-L6-v2',
    eval_model: str = "mistralai/Mistral-7B-Instruct-v0.2",
    load_in_8bit: bool = False,
    print_results: bool = True
) -> Dict[str, Any]:
    """
    Run evaluation on a single question-answer pair.
    
    Args:
        question: The question
        model_answer: The model's answer
        correct_answer: The correct answer (if available)
        context: List of context strings (if available)
        embedding_model: Name of the embedding model to use
        eval_model: Name of the model to use for deep evaluation metrics
        load_in_8bit: Whether to load the evaluation model in 8-bit precision
        print_results: Whether to print the results
        
    Returns:
        Dictionary of evaluation metrics
    """
    # Initialize evaluator
    evaluator = QAEvaluator(
        embedding_model_name=embedding_model,
        eval_model_name=eval_model,
        load_in_8bit=load_in_8bit
    )
    
    # Run evaluation
    metrics = evaluator.evaluate(
        question=question,
        model_answer=model_answer,
        correct_answer=correct_answer,
        context=context
    )
    
    # Print results if requested
    if print_results:
        print("\nEvaluation Results:")
        for metric, value in metrics.items():
            if isinstance(value, float):
                print(f"{metric}: {value:.4f}")
            elif not isinstance(value, str) or len(value) < 100:
                print(f"{metric}: {value}")
            else:
                print(f"{metric}: {value[:100]}...")
    
    return metrics


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Evaluate metrics for question-answer pairs using local models',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--question', type=str, required=True,
                        help='The question to evaluate')
    parser.add_argument('--answer', type=str, required=True,
                        help='The model\'s answer')
    parser.add_argument('--correct-answer', type=str, default=None,
                        help='The correct answer (if available)')
    parser.add_argument('--context', type=str, nargs='+', default=None,
                        help='Context information (if available)')
    parser.add_argument('--embedding-model', type=str, default='paraphrase-MiniLM-L6-v2',
                        help='Name of the embedding model to use')
    parser.add_argument('--eval-model', type=str, default="mistralai/Mistral-7B-Instruct-v0.2",
                        help='Hugging Face model to use for evaluation metrics')
    parser.add_argument('--load-in-8bit', action='store_true',
                        help='Whether to load the evaluation model in 8-bit precision')
    
    args = parser.parse_args()
    
    # Run evaluation
    metrics = run_evaluation(
        question=args.question,
        model_answer=args.answer,
        correct_answer=args.correct_answer,
        context=args.context,
        embedding_model=args.embedding_model,
        eval_model=args.eval_model,
        load_in_8bit=args.load_in_8bit
    ) 